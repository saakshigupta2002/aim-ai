# References
> [!TIP]
> List of references used by us
1. LoRA paper: https://arxiv.org/abs/2106.09685
2. [Model Training Parameter graphs](https://wandb.ai/akshatshaw-iitr/huggingface/reports/FineTuning-mistral-model--Vmlldzo3NDk2ODQ3?accessToken=sp6lqo5qjh405qm3zuo1jhr2wnmdqqvaj03ejxyz4txoj8pweileedlh3kxk33vm)
3. [Fine tuning of LLMs - Guide](https://www.e2enetworks.com/blog/a-step-by-step-guide-to-fine-tuning-the-mistral-7b-llm)
4. [BleU evaluation metric](https://huggingface.co/spaces/evaluate-metric/bleu)
5. [Dataset1](https://github.com/OmdenaAI/omdena-hyderabad-Chatbot-for-interview/blob/main/src/data/originals/Customer%20Service%20Representative.csv), [Dataset2](https://github.com/OmdenaAI/omdena-hyderabad-Chatbot-for-interview/blob/main/src/data/originals/Final%20Dataset%20Team%203.csv)
6. [Mistral Interview Model](https://huggingface.co/akshatshaw/mistral-interview-finetune)
7. [Phi2-Api intially made, which was then replaced by our existing model](https://www.kaggle.com/code/agampy/phi2-api)
8. [Phi2-Peft](https://www.kaggle.com/code/agampy/phi-2-peft)
9. [ASR-functions-notebook](https://www.kaggle.com/code/paradoxplusparadise/asr-techshila)
10. [Faster-whisper-github](https://github.com/SYSTRAN/faster-whisper)
