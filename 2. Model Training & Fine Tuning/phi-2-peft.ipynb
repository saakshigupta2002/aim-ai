{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-10T04:47:01.626967Z","iopub.status.busy":"2024-04-10T04:47:01.626603Z","iopub.status.idle":"2024-04-10T04:47:03.404842Z","shell.execute_reply":"2024-04-10T04:47:03.403827Z","shell.execute_reply.started":"2024-04-10T04:47:01.626932Z"},"papermill":{"duration":0.730601,"end_time":"2024-01-20T10:20:57.894357","exception":false,"start_time":"2024-01-20T10:20:57.163756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:47:03.406712Z","iopub.status.busy":"2024-04-10T04:47:03.406187Z","iopub.status.idle":"2024-04-10T04:47:40.434926Z","shell.execute_reply":"2024-04-10T04:47:40.433667Z","shell.execute_reply.started":"2024-04-10T04:47:03.406681Z"},"papermill":{"duration":21.139912,"end_time":"2024-01-20T10:21:19.149213","exception":false,"start_time":"2024-01-20T10:20:58.009301","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\n","libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n","ydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:47:40.438543Z","iopub.status.busy":"2024-04-10T04:47:40.438149Z","iopub.status.idle":"2024-04-10T04:47:40.444178Z","shell.execute_reply":"2024-04-10T04:47:40.443002Z","shell.execute_reply.started":"2024-04-10T04:47:40.438507Z"},"papermill":{"duration":0.020442,"end_time":"2024-01-20T10:21:19.183228","exception":false,"start_time":"2024-01-20T10:21:19.162786","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","# disable Weights and Biases\n","os.environ['WANDB_DISABLED']=\"true\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:47:40.445889Z","iopub.status.busy":"2024-04-10T04:47:40.445524Z","iopub.status.idle":"2024-04-10T04:48:25.023665Z","shell.execute_reply":"2024-04-10T04:48:25.022406Z","shell.execute_reply.started":"2024-04-10T04:47:40.445845Z"},"papermill":{"duration":18.720742,"end_time":"2024-01-20T10:21:37.916969","exception":true,"start_time":"2024-01-20T10:21:19.196227","status":"failed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-10 04:47:57.915035: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-10 04:47:57.915195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-10 04:47:58.184268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"]},{"name":"stdout","output_type":"stream","text":["Enter your token (input will not be visible):  ·····································\n","Add token as git credential? (Y/n)  Y\n"]},{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n","    GenerationConfig\n",")\n","from tqdm import tqdm\n","from trl import SFTTrainer\n","import torch\n","import time\n","import pandas as pd\n","import numpy as np\n","from huggingface_hub import interpreter_login\n","\n","interpreter_login()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:48:25.028212Z","iopub.status.busy":"2024-04-10T04:48:25.025177Z","iopub.status.idle":"2024-04-10T04:48:25.035992Z","shell.execute_reply":"2024-04-10T04:48:25.034739Z","shell.execute_reply.started":"2024-04-10T04:48:25.028176Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["from pynvml import *\n","\n","def print_gpu_utilization():\n","    nvmlInit()\n","    handle = nvmlDeviceGetHandleByIndex(0)\n","    info = nvmlDeviceGetMemoryInfo(handle)\n","    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='2'></a>\n","#### 2. Loading dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:48:25.038314Z","iopub.status.busy":"2024-04-10T04:48:25.037591Z","iopub.status.idle":"2024-04-10T04:48:27.189270Z","shell.execute_reply":"2024-04-10T04:48:27.188185Z","shell.execute_reply.started":"2024-04-10T04:48:25.038273Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|██████████| 1.10M/1.10M [00:00<00:00, 3.74MB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1eac568444184c1986e7654b3f679606","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Unnamed: 0', 'Job Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality'],\n","        num_rows: 2323\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# https://huggingface.co/datasets/neil-code/dialogsum-test\n","huggingface_dataset_name = \"AgamP/techshila_ml\"\n","dataset = load_dataset(huggingface_dataset_name)\n","dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:48:27.191325Z","iopub.status.busy":"2024-04-10T04:48:27.190802Z","iopub.status.idle":"2024-04-10T04:48:27.214773Z","shell.execute_reply":"2024-04-10T04:48:27.213704Z","shell.execute_reply.started":"2024-04-10T04:48:27.191285Z"},"trusted":true},"outputs":[],"source":["dataset=dataset['train'].train_test_split(test_size=0.3)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["This is what the data looks like:"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:48:27.216639Z","iopub.status.busy":"2024-04-10T04:48:27.216180Z","iopub.status.idle":"2024-04-10T04:48:27.228618Z","shell.execute_reply":"2024-04-10T04:48:27.227475Z","shell.execute_reply.started":"2024-04-10T04:48:27.216595Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Unnamed: 0', 'Job Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality'],\n","        num_rows: 1626\n","    })\n","    test: Dataset({\n","        features: ['Unnamed: 0', 'Job Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality'],\n","        num_rows: 697\n","    })\n","})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:12:12.434273Z","iopub.status.busy":"2024-04-10T05:12:12.433861Z","iopub.status.idle":"2024-04-10T05:12:12.443531Z","shell.execute_reply":"2024-04-10T05:12:12.442377Z","shell.execute_reply.started":"2024-04-10T05:12:12.434241Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["{'Unnamed: 0': 2037,\n"," 'Job Position': 'Nurse',\n"," 'Question': 'what would you do if a patient complains about their pain not being managed effectively',\n"," 'Answer': \"The primary step is to empathetically listen to the patient and affirm their experience. A comprehensive pain evaluation would be performed to comprehend the intensity and nature of their discomfort. This information would be shared with the patient's medical team to revisit and potentially revise their pain control plan. It's crucial to keep the patient informed about any modifications and engage them in the decisionmaking process fostering a sense of mutual trust and control.\",\n"," 'Interview Phase': 'Communication',\n"," 'Answer Quality': 'Unrated'}"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='3'></a>\n","#### 3. Create bitsandbytes configuration\n","\n","**To load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll achieve this with BitesAndBytesConfig from the Transformers library. This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices.**"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:48:27.257449Z","iopub.status.busy":"2024-04-10T04:48:27.257024Z","iopub.status.idle":"2024-04-10T04:48:27.266960Z","shell.execute_reply":"2024-04-10T04:48:27.265945Z","shell.execute_reply.started":"2024-04-10T04:48:27.257416Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=False,\n","    )\n","device_map = {\"\": 0}"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='4'></a>\n","#### 4. Load Base Model\n","Let's now load Phi-2 using 4-bit quantization!"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:48:27.268765Z","iopub.status.busy":"2024-04-10T04:48:27.268368Z","iopub.status.idle":"2024-04-10T04:48:41.824226Z","shell.execute_reply":"2024-04-10T04:48:41.823002Z","shell.execute_reply.started":"2024-04-10T04:48:27.268730Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install accelerate"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:48:41.826183Z","iopub.status.busy":"2024-04-10T04:48:41.825835Z","iopub.status.idle":"2024-04-10T04:49:27.492433Z","shell.execute_reply":"2024-04-10T04:49:27.491044Z","shell.execute_reply.started":"2024-04-10T04:48:41.826149Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7bd5cacbd29740558ad3a0cf623c33a3","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32775a7aec454e73b89d1cb0d38d4137","version_major":2,"version_minor":0},"text/plain":["configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n","- configuration_phi.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e89309c005e46b6a42b3e95ea9fad9b","version_major":2,"version_minor":0},"text/plain":["modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n","- modeling_phi.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9136722c99e4421a64a382438c61174","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cef96d9427a440668c06dcd415659b57","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"946b7153ca4147cea81811958978a785","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34642c3cbe2b48c2b28618b48a97caf8","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e831666bdf1144ada2f75947806a2ca3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7d37790275c48e18bf97339432d719b","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name='microsoft/phi-2'\n","original_model = AutoModelForCausalLM.from_pretrained(model_name, \n","                                                      device_map=device_map,\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='5'></a>\n","#### 5. Tokenization\n","Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa)."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:49:27.495831Z","iopub.status.busy":"2024-04-10T04:49:27.495383Z","iopub.status.idle":"2024-04-10T04:49:29.868541Z","shell.execute_reply":"2024-04-10T04:49:29.867399Z","shell.execute_reply.started":"2024-04-10T04:49:27.495790Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29f0ce0a1d454dde9a3e6cd4ae8619b4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a023c8dc126544429629dcb510923f9f","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b2aca197ae943d19cb4678c42550f03","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43ab9b15fbf44d4b902783f2d1fbc036","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ca19cefb407478cb59a0afafc913f02","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55f9661f04ee4804b9ba5b809d8e53a1","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n","tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:49:29.870852Z","iopub.status.busy":"2024-04-10T04:49:29.870435Z","iopub.status.idle":"2024-04-10T04:49:30.051731Z","shell.execute_reply":"2024-04-10T04:49:30.050516Z","shell.execute_reply.started":"2024-04-10T04:49:29.870814Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token\n","\n","def gen(model,p, maxlen=1000, sample=True):\n","    toks = eval_tokenizer(p, return_tensors=\"pt\")\n","    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n","    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='6'></a>\n","#### 6. Test the Model with Zero Shot Inferencing"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:49:30.053321Z","iopub.status.busy":"2024-04-10T04:49:30.053006Z","iopub.status.idle":"2024-04-10T04:49:43.712296Z","shell.execute_reply":"2024-04-10T04:49:43.711216Z","shell.execute_reply.started":"2024-04-10T04:49:30.053295Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: You are an excellent interviewer for the given job role Nurse and your task is to ask relevant questions to the candidate similar to how do you stay up to date on medical advancements and procedures\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","Answer:\n","I stay current by reading medical journals attending conferences and participating in online learning modules. I also network with colleagues and discuss new developments with physicians and nurses.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","1. How do you stay up to date on the latest medical advancements and procedures?\n","2. Can you describe a time when you had to quickly learn a new medical procedure?\n","3. How do you ensure that you are providing the best possible care to your patients?\n","4. How do you handle difficult or stressful situations in a medical setting?\n","5. How do you stay organized and manage your time effectively in a fast-paced environment?\n","6. Can you describe a time when\n","CPU times: user 8.69 s, sys: 288 ms, total: 8.98 s\n","Wall time: 10.4 s\n"]}],"source":["%%time\n","from transformers import set_seed\n","seed = 42\n","set_seed(seed)\n","\n","index = 10\n","\n","prompt = dataset['train'][index]['Question']\n","summary = dataset['train'][index]['Answer']\n","job_role=dataset['train'][index]['Job Position']\n","interview_phase=dataset['train'][index]['Interview Phase']\n","\n","formatted_prompt = f\"Instruct: You are an excellent interviewer for the given job role {job_role} and your task is to ask relevant questions to the candidate similar to {prompt}\\nOutput:\\n\"\n","res = gen(original_model,formatted_prompt,100,)\n","#print(res[0])\n","output = res[0].split('Output:\\n')[1]\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{formatted_prompt}')\n","print(dash_line)\n","print(f'Answer:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='7'></a>\n","#### 7. Pre-processing dataset"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:49:43.714042Z","iopub.status.busy":"2024-04-10T04:49:43.713711Z","iopub.status.idle":"2024-04-10T04:49:43.722286Z","shell.execute_reply":"2024-04-10T04:49:43.721232Z","shell.execute_reply.started":"2024-04-10T04:49:43.714013Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["def create_prompt_formats(sample):\n","    \"\"\"\n","    Format various fields of the sample ('instruction','output')\n","    Then concatenate them using two newline characters \n","    :param sample: Sample dictionnary\n","    \"\"\"\n","    intro_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n","    INSTRUCTION_KEY = \"### Instruct: You are an excellent interviewer for Job Roles mentioned by the candidate, Your task is to evaluate the answers by the candidate and generate questions relevant to the Job Role while keeping in mind the interview phase i.e. General, Technical, Behavioural,etc\"\n","    RESPONSE_KEY = \"### Output:\"\n","    END_KEY = \"### End\"\n","    \n","    intro = f\"\\n{intro_prompt}\"\n","    instruction = f\"{INSTRUCTION_KEY}\"\n","    input_context = f\"{sample['Job Position']}\"+f\"{sample['Interview Phase']}\"+f\"{sample['Question']}\" if sample[\"Question\"] else None\n","    response = f\"{RESPONSE_KEY}\\n{sample['Answer']}\"\n","    end = f\"{END_KEY}\"\n","    \n","    parts = [part for part in [intro, instruction, input_context, response, end] if part]\n","\n","    formatted_prompt = \"\\n\\n\".join(parts)\n","    sample[\"text\"] = formatted_prompt\n","\n","    return sample"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:49:43.723986Z","iopub.status.busy":"2024-04-10T04:49:43.723700Z","iopub.status.idle":"2024-04-10T04:49:43.737181Z","shell.execute_reply":"2024-04-10T04:49:43.736046Z","shell.execute_reply.started":"2024-04-10T04:49:43.723961Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n","def get_max_length(model):\n","    conf = model.config\n","    max_length = None\n","    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n","        max_length = getattr(model.config, length_setting, None)\n","        if max_length:\n","            print(f\"Found max lenth: {max_length}\")\n","            break\n","    if not max_length:\n","        max_length = 1024\n","        print(f\"Using default max length: {max_length}\")\n","    return max_length\n","\n","\n","def preprocess_batch(batch, tokenizer, max_length):\n","    \"\"\"\n","    Tokenizing a batch\n","    \"\"\"\n","    return tokenizer(\n","        batch[\"text\"],\n","        max_length=max_length,\n","        truncation=True,\n","    )"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:13.169170Z","iopub.status.busy":"2024-04-10T04:50:13.168157Z","iopub.status.idle":"2024-04-10T04:50:13.176145Z","shell.execute_reply":"2024-04-10T04:50:13.175158Z","shell.execute_reply.started":"2024-04-10T04:50:13.169117Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["from functools import partial\n","\n","# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n","def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n","    \"\"\"Format & tokenize it so it is ready for training\n","    :param tokenizer (AutoTokenizer): Model Tokenizer\n","    :param max_length (int): Maximum number of tokens to emit from tokenizer\n","    \"\"\"\n","    \n","    # Add prompt to each sample\n","    print(\"Preprocessing dataset...\")\n","    dataset = dataset.map(create_prompt_formats)#, batched=True)\n","    \n","    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n","    dataset = dataset.map(\n","        _preprocessing_function,\n","        batched=True,\n","        #remove_columns=['id', 'topic', 'dialogue', 'summary'],\n","    )\n","\n","    # Filter out samples that have input_ids exceeding max_length\n","    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n","    \n","    # Shuffle dataset\n","    dataset = dataset.shuffle(seed=seed)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:13.551981Z","iopub.status.busy":"2024-04-10T04:50:13.551031Z","iopub.status.idle":"2024-04-10T04:50:25.317833Z","shell.execute_reply":"2024-04-10T04:50:25.316789Z","shell.execute_reply.started":"2024-04-10T04:50:13.551945Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found max lenth: 2048\n","2048\n","Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73b6454aa971464fb99221461c6c9882","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1626 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d58c8d8f800e4f4ab45a20a817c982f3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1626 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82fc3f3f9a9a421cb26bbad64716a28d","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1626 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebd88167bed64d62bfbc2fcb21583e15","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/697 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e63f3f37903f4916ad499d2e0500ce7f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/697 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"056bed323d4b420aac0fd62b5a2093ec","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/697 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# ## Pre-process dataset\n","max_length = get_max_length(original_model)\n","print(max_length)\n","\n","train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n","eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['test'])"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:25.320599Z","iopub.status.busy":"2024-04-10T04:50:25.319930Z","iopub.status.idle":"2024-04-10T04:50:25.325729Z","shell.execute_reply":"2024-04-10T04:50:25.324716Z","shell.execute_reply.started":"2024-04-10T04:50:25.320544Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shapes of the datasets:\n","Training: (1626, 9)\n","Validation: (697, 9)\n"]}],"source":["print(f\"Shapes of the datasets:\")\n","print(f\"Training: {train_dataset.shape}\")\n","print(f\"Validation: {eval_dataset.shape}\")\n","#print(train_dataset)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='8'></a>\n","#### 8. Setup the PEFT/LoRA model for Fine-Tuning"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:25.327405Z","iopub.status.busy":"2024-04-10T04:50:25.327043Z","iopub.status.idle":"2024-04-10T04:50:25.345986Z","shell.execute_reply":"2024-04-10T04:50:25.345144Z","shell.execute_reply.started":"2024-04-10T04:50:25.327372Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 262364160\n","all model parameters: 1521392640\n","percentage of trainable model parameters: 17.24%\n"]}],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","\n","print(print_number_of_trainable_model_parameters(original_model))"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:25.348156Z","iopub.status.busy":"2024-04-10T04:50:25.347883Z","iopub.status.idle":"2024-04-10T04:50:25.363301Z","shell.execute_reply":"2024-04-10T04:50:25.362410Z","shell.execute_reply.started":"2024-04-10T04:50:25.348133Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiAttention(\n","          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")\n"]}],"source":["print(original_model)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:25.364678Z","iopub.status.busy":"2024-04-10T04:50:25.364403Z","iopub.status.idle":"2024-04-10T04:50:25.887824Z","shell.execute_reply":"2024-04-10T04:50:25.886929Z","shell.execute_reply.started":"2024-04-10T04:50:25.364654Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","config = LoraConfig(\n","    r=32, #Rank\n","    lora_alpha=32,\n","    target_modules=[\n","        'q_proj',\n","        'k_proj',\n","        'v_proj',\n","        'dense'\n","    ],\n","    bias=\"none\",\n","    lora_dropout=0.05,  # Conventional\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n","original_model.gradient_checkpointing_enable()\n","\n","# 2 - Using the prepare_model_for_kbit_training method from PEFT\n","original_model = prepare_model_for_kbit_training(original_model)\n","\n","peft_model = get_peft_model(original_model, config)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:25.889619Z","iopub.status.busy":"2024-04-10T04:50:25.889191Z","iopub.status.idle":"2024-04-10T04:50:25.902458Z","shell.execute_reply":"2024-04-10T04:50:25.901368Z","shell.execute_reply.started":"2024-04-10T04:50:25.889584Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 20971520\n","all model parameters: 1542364160\n","percentage of trainable model parameters: 1.36%\n"]}],"source":["print(print_number_of_trainable_model_parameters(peft_model))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:50:25.904027Z","iopub.status.busy":"2024-04-10T04:50:25.903719Z","iopub.status.idle":"2024-04-10T04:50:25.925876Z","shell.execute_reply":"2024-04-10T04:50:25.924911Z","shell.execute_reply.started":"2024-04-10T04:50:25.904001Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): PhiForCausalLM(\n","      (model): PhiModel(\n","        (embed_tokens): Embedding(51200, 2560)\n","        (embed_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-31): 32 x PhiDecoderLayer(\n","            (self_attn): PhiAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (dense): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (rotary_emb): PhiRotaryEmbedding()\n","            )\n","            (mlp): PhiMLP(\n","              (activation_fn): NewGELUActivation()\n","              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","            )\n","            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["print(peft_model)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='9'></a>\n","#### 9. Train PEFT Adapter\n","\n","Define training arguments and create Trainer instance."]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:51:02.581689Z","iopub.status.busy":"2024-04-10T04:51:02.580660Z","iopub.status.idle":"2024-04-10T04:51:02.627649Z","shell.execute_reply":"2024-04-10T04:51:02.626274Z","shell.execute_reply.started":"2024-04-10T04:51:02.581647Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["output_dir = './peft-dialogue-summary-training/final-checkpoint'\n","import transformers\n","\n","peft_training_args = TrainingArguments(\n","    output_dir = output_dir,\n","    warmup_steps=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    max_steps=5,\n","    learning_rate=2e-4,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=1,\n","    logging_dir=\"./logs\",\n","    save_strategy=\"steps\",\n","    save_steps=1,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=1,\n","    do_eval=True,\n","    gradient_checkpointing=True,\n","    report_to=\"none\",\n","    overwrite_output_dir = 'True',\n","    group_by_length=True,\n",")\n","\n","peft_model.config.use_cache = False\n","\n","peft_trainer = transformers.Trainer(\n","    model=peft_model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    args=peft_training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:51:04.419251Z","iopub.status.busy":"2024-04-10T04:51:04.418854Z","iopub.status.idle":"2024-04-10T04:51:04.425577Z","shell.execute_reply":"2024-04-10T04:51:04.424577Z","shell.execute_reply.started":"2024-04-10T04:51:04.419219Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["peft_training_args.device"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T04:51:05.650733Z","iopub.status.busy":"2024-04-10T04:51:05.650039Z","iopub.status.idle":"2024-04-10T05:02:53.217454Z","shell.execute_reply":"2024-04-10T05:02:53.216336Z","shell.execute_reply.started":"2024-04-10T04:51:05.650697Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5/5 11:35, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.191000</td>\n","      <td>2.935906</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.445700</td>\n","      <td>2.847509</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.629500</td>\n","      <td>2.763322</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.564500</td>\n","      <td>2.710619</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.647700</td>\n","      <td>2.679134</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=5, training_loss=2.495686435699463, metrics={'train_runtime': 706.4528, 'train_samples_per_second': 0.057, 'train_steps_per_second': 0.007, 'total_flos': 182343221821440.0, 'train_loss': 2.495686435699463, 'epoch': 0.02})"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["peft_trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:02:53.228038Z","iopub.status.idle":"2024-04-10T05:02:53.228594Z","shell.execute_reply":"2024-04-10T05:02:53.228313Z","shell.execute_reply.started":"2024-04-10T05:02:53.228291Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["print_gpu_utilization()"]},{"cell_type":"markdown","metadata":{},"source":["Saving the trained adapter"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os \n","os.chdir(r'/kaggle/working')\n","from IPython.display import FileLink \n","FileLink(r'peft-dialogue-summary-training')"]},{"cell_type":"markdown","metadata":{},"source":["Pushing model on huggingface repo"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:09:18.904280Z","iopub.status.busy":"2024-04-10T05:09:18.903367Z","iopub.status.idle":"2024-04-10T05:09:30.133680Z","shell.execute_reply":"2024-04-10T05:09:30.132568Z","shell.execute_reply.started":"2024-04-10T05:09:18.904233Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"]},{"name":"stdout","output_type":"stream","text":["Enter your token (input will not be visible):  ·····································\n","Add token as git credential? (Y/n)  Y\n"]},{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import interpreter_login\n","\n","interpreter_login()"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:10:32.437917Z","iopub.status.busy":"2024-04-10T05:10:32.437101Z","iopub.status.idle":"2024-04-10T05:10:33.715077Z","shell.execute_reply":"2024-04-10T05:10:33.714051Z","shell.execute_reply.started":"2024-04-10T05:10:32.437884Z"},"trusted":true},"outputs":[{"data":{"text/plain":["RepoUrl('https://huggingface.co/AgamP/phi-2-PEFT-e5', endpoint='https://huggingface.co', repo_type='model', repo_id='AgamP/phi-2-PEFT-e5')"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import create_repo\n","create_repo(\"phi-2-PEFT-e5\")"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:15:57.959898Z","iopub.status.busy":"2024-04-10T05:15:57.959463Z","iopub.status.idle":"2024-04-10T05:16:02.869223Z","shell.execute_reply":"2024-04-10T05:16:02.868177Z","shell.execute_reply.started":"2024-04-10T05:15:57.959865Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50ac264c074f4e8f8236e2e14e7f36ef","version_major":2,"version_minor":0},"text/plain":["Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0e632e5aec0424f88e4c67955da6a3c","version_major":2,"version_minor":0},"text/plain":["scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7202ee0223d24acb903b9545cc1ae777","version_major":2,"version_minor":0},"text/plain":["optimizer.pt:   0%|          | 0.00/42.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fef357ca7a3a4ac2b582992b835f0bce","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5226bb52ef0b4c7dacf58e72b0eb5cfd","version_major":2,"version_minor":0},"text/plain":["rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7b7e9a0cab444b78f1eaeb930b8e302","version_major":2,"version_minor":0},"text/plain":["training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/AgamP/phi-2-PEFT-e5/commit/e73e4a074da18c3872e44cd0dc852a159119af0d', commit_message='Upload folder using huggingface_hub', commit_description='', oid='e73e4a074da18c3872e44cd0dc852a159119af0d', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi\n","\n","api=HfApi()\n","\n","api.upload_folder(\n","    folder_path=\"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-5\",\n","    repo_id=\"AgamP/phi-2-PEFT-e5\",\n","    repo_type=\"model\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T10:12:15.717375Z","iopub.status.busy":"2024-01-20T10:12:15.716634Z","iopub.status.idle":"2024-01-20T10:12:16.006683Z","shell.execute_reply":"2024-01-20T10:12:16.005744Z","shell.execute_reply.started":"2024-01-20T10:12:15.717341Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["# Free memory for merging weights\n","del original_model\n","del peft_trainer\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T10:12:19.432308Z","iopub.status.busy":"2024-01-20T10:12:19.431935Z","iopub.status.idle":"2024-01-20T10:12:19.437916Z","shell.execute_reply":"2024-01-20T10:12:19.436995Z","shell.execute_reply.started":"2024-01-20T10:12:19.432277Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["print_gpu_utilization()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='10'></a>\n","#### 10. Evaluate the Model Qualitatively (Human Evaluation)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:25:21.085311Z","iopub.status.busy":"2024-04-10T05:25:21.084490Z","iopub.status.idle":"2024-04-10T05:25:25.112640Z","shell.execute_reply":"2024-04-10T05:25:25.111695Z","shell.execute_reply.started":"2024-04-10T05:25:21.085273Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0bec419fe2047a19525eb3b9affdf97","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","base_model_id = \"microsoft/phi-2\"\n","base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:25:30.026931Z","iopub.status.busy":"2024-04-10T05:25:30.026528Z","iopub.status.idle":"2024-04-10T05:25:30.215303Z","shell.execute_reply":"2024-04-10T05:25:30.214408Z","shell.execute_reply.started":"2024-04-10T05:25:30.026901Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:25:40.853528Z","iopub.status.busy":"2024-04-10T05:25:40.852510Z","iopub.status.idle":"2024-04-10T05:25:41.480010Z","shell.execute_reply":"2024-04-10T05:25:41.479034Z","shell.execute_reply.started":"2024-04-10T05:25:40.853481Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["from peft import PeftModel\n","\n","ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-5\",torch_dtype=torch.float16,is_trainable=False)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:28:05.267237Z","iopub.status.busy":"2024-04-10T05:28:05.266806Z","iopub.status.idle":"2024-04-10T05:28:05.446096Z","shell.execute_reply":"2024-04-10T05:28:05.444951Z","shell.execute_reply.started":"2024-04-10T05:28:05.267204Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 4 µs, sys: 2 µs, total: 6 µs\n","Wall time: 11.9 µs\n","---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: You are an excellent interviewer for the given job role Data Scientist and your task is to ask relevant questions to the candidate similar to Describe your experience with model explainability techniques and their importance in machine learning projects.\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","Answer:\n","I have experience with model explainability techniques such as feature importance analysis, partial dependence plots, SHAP values, and LIME (Local Interpretable Model-agnostic Explanations). These techniques help understand the contribution of features to model predictions, identify patterns and correlations, and explain the decision-making process of complex machine learning models to stakeholders. Model explainability enhances transparency, trust, and accountability in machine learning projects, enabling stakeholders to understand, validate, and interpret model outputs.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","\n"]}],"source":["%time\n","from transformers import set_seed\n","seed = 42\n","set_seed(seed)\n","\n","index = 1000\n","\n","prompt = dataset['train'][index]['Question']\n","summary = dataset['train'][index]['Answer']\n","job_role=dataset['train'][index]['Job Position']\n","interview_phase=dataset['train'][index]['Interview Phase']\n","\n","formatted_prompt = f\"Instruct: You are an excellent interviewer for the given job role {job_role} and your task is to ask relevant questions to the candidate similar to {prompt}\\nOutput:\\n\"\n","res = gen(original_model,formatted_prompt,100,)\n","#print(res[0])\n","output = res[0].split('Output:\\n')[1]\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{formatted_prompt}')\n","print(dash_line)\n","print(f'Answer:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"source":["<a name='11'></a>\n","#### 11. Evaluate the Model Quantitatively (with ROUGE Metric)\n","Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). "]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:28:31.187939Z","iopub.status.busy":"2024-04-10T05:28:31.187560Z","iopub.status.idle":"2024-04-10T05:28:35.308392Z","shell.execute_reply":"2024-04-10T05:28:35.307065Z","shell.execute_reply.started":"2024-04-10T05:28:31.187911Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"300d3aec042e49379abb2de23bbdab72","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset[]"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T06:15:44.985684Z","iopub.status.busy":"2024-04-10T06:15:44.985134Z","iopub.status.idle":"2024-04-10T06:15:45.020575Z","shell.execute_reply":"2024-04-10T06:15:45.019516Z","shell.execute_reply.started":"2024-04-10T06:15:44.985650Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Job Position</th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>Interview Phase</th>\n","      <th>Answer Quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Customer Service Representative</td>\n","      <td>What are Your Biggest Achievements?</td>\n","      <td>During my last job, I¬†learned¬†some interpers...</td>\n","      <td>General</td>\n","      <td>Average</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Customer Service Representative</td>\n","      <td>Name any Two Improvements You Made in the Prev...</td>\n","      <td>As a few of my team members were late to work,...</td>\n","      <td>General</td>\n","      <td>Good</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Customer Service Representative</td>\n","      <td>Tell me about a professional accomplishment yo...</td>\n","      <td>One of my proudest professional accomplishment...</td>\n","      <td>General</td>\n","      <td>Good</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Customer Service Representative</td>\n","      <td>Have you ever utilized customer feedback to en...</td>\n","      <td>Yes, I've leveraged customer feedback to impro...</td>\n","      <td>Role Specific</td>\n","      <td>Average</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Customer Service Representative</td>\n","      <td>Have You Used Customer Feedback to Ensure Busi...</td>\n","      <td>Yes, I have used customer feedback to improve ...</td>\n","      <td>Role Specific</td>\n","      <td>Good</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                     Job Position  \\\n","0           0  Customer Service Representative   \n","1           1  Customer Service Representative   \n","2           2  Customer Service Representative   \n","3           3  Customer Service Representative   \n","4           4  Customer Service Representative   \n","\n","                                            Question  \\\n","0                What are Your Biggest Achievements?   \n","1  Name any Two Improvements You Made in the Prev...   \n","2  Tell me about a professional accomplishment yo...   \n","3  Have you ever utilized customer feedback to en...   \n","4  Have You Used Customer Feedback to Ensure Busi...   \n","\n","                                              Answer Interview Phase  \\\n","0  During my last job, I¬†learned¬†some interpers...         General   \n","1  As a few of my team members were late to work,...         General   \n","2  One of my proudest professional accomplishment...         General   \n","3  Yes, I've leveraged customer feedback to impro...   Role Specific   \n","4  Yes, I have used customer feedback to improve ...   Role Specific   \n","\n","  Answer Quality  \n","0        Average  \n","1           Good  \n","2           Good  \n","3        Average  \n","4           Good  "]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["df1=pd.read_csv(\"/kaggle/input/combined-data-techshila/data (1).csv\")\n","df1.head()"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:59:52.938470Z","iopub.status.busy":"2024-04-10T05:59:52.937530Z","iopub.status.idle":"2024-04-10T05:59:52.947886Z","shell.execute_reply":"2024-04-10T05:59:52.946847Z","shell.execute_reply.started":"2024-04-10T05:59:52.938431Z"},"trusted":true},"outputs":[],"source":["questions = dataset['test'][595:600]['Question']\n","job_position=dataset['test'][595:600]['Job Position']\n","interview_phase=dataset['test'][595:600]['Interview Phase']\n","answer_quality=dataset['test'][595:600]['Answer Quality']\n","answer = dataset['test'][595:600]['Answer']"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:59:57.509200Z","iopub.status.busy":"2024-04-10T05:59:57.508803Z","iopub.status.idle":"2024-04-10T05:59:57.516085Z","shell.execute_reply":"2024-04-10T05:59:57.514967Z","shell.execute_reply.started":"2024-04-10T05:59:57.509172Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['why did you become a nursing assistant',\n"," 'what would you do if you had a difficult patient',\n"," 'how would you deal with a rude patient',\n"," 'tell us about a time when you had to do something difficult how did you handle that and how can that help you as a Certified nursing assistants',\n"," 'Can you discuss the challenges of working with big data and techniques to handle scalability, performance, and resource constraints?']"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["questions"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T06:19:50.428234Z","iopub.status.busy":"2024-04-10T06:19:50.427464Z","iopub.status.idle":"2024-04-10T06:19:50.455638Z","shell.execute_reply":"2024-04-10T06:19:50.454367Z","shell.execute_reply.started":"2024-04-10T06:19:50.428198Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Job Position</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>790</th>\n","      <td>why did you decide on a career as a nurse</td>\n","      <td>Nurse</td>\n","    </tr>\n","    <tr>\n","      <th>791</th>\n","      <td>why did you decide on a career as a nurse</td>\n","      <td>Nurse</td>\n","    </tr>\n","    <tr>\n","      <th>792</th>\n","      <td>why did you decide on a career as a nurse</td>\n","      <td>Nurse</td>\n","    </tr>\n","    <tr>\n","      <th>793</th>\n","      <td>why did you decide on a career as a nurse</td>\n","      <td>Nurse</td>\n","    </tr>\n","    <tr>\n","      <th>794</th>\n","      <td>why did you decide on a career as a nurse</td>\n","      <td>Nurse</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2247</th>\n","      <td>Describe your experience with deep learning ar...</td>\n","      <td>Data Scientist</td>\n","    </tr>\n","    <tr>\n","      <th>2248</th>\n","      <td>How do you approach building interpretable mac...</td>\n","      <td>Data Scientist</td>\n","    </tr>\n","    <tr>\n","      <th>2249</th>\n","      <td>Can you explain the concept of feature selecti...</td>\n","      <td>Data Scientist</td>\n","    </tr>\n","    <tr>\n","      <th>2250</th>\n","      <td>Describe your experience with Bayesian statist...</td>\n","      <td>Data Scientist</td>\n","    </tr>\n","    <tr>\n","      <th>2251</th>\n","      <td>How do you approach building predictive models...</td>\n","      <td>Data Scientist</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1232 rows × 2 columns</p>\n","</div>"],"text/plain":["                                               Question    Job Position\n","790           why did you decide on a career as a nurse           Nurse\n","791           why did you decide on a career as a nurse           Nurse\n","792           why did you decide on a career as a nurse           Nurse\n","793           why did you decide on a career as a nurse           Nurse\n","794           why did you decide on a career as a nurse           Nurse\n","...                                                 ...             ...\n","2247  Describe your experience with deep learning ar...  Data Scientist\n","2248  How do you approach building interpretable mac...  Data Scientist\n","2249  Can you explain the concept of feature selecti...  Data Scientist\n","2250  Describe your experience with Bayesian statist...  Data Scientist\n","2251  How do you approach building predictive models...  Data Scientist\n","\n","[1232 rows x 2 columns]"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["ground_truth_questions=df1.loc[df1['Job Position'].isin(job_position),['Question','Job Position']]\n","ground_truth_questions"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T06:29:22.000819Z","iopub.status.busy":"2024-04-10T06:29:22.000345Z","iopub.status.idle":"2024-04-10T06:29:22.011865Z","shell.execute_reply":"2024-04-10T06:29:22.010576Z","shell.execute_reply.started":"2024-04-10T06:29:22.000791Z"},"trusted":true},"outputs":[{"data":{"text/plain":["790             why did you decide on a career as a nurse\n","791             why did you decide on a career as a nurse\n","792             why did you decide on a career as a nurse\n","793             why did you decide on a career as a nurse\n","794             why did you decide on a career as a nurse\n","                              ...                        \n","2047    what techniques would you use to calm an upset...\n","2048    can you recall an episode at work where you ma...\n","2049    in what ways has your prior work history equip...\n","2050    can you describe a situation where you had to ...\n","2051    how would you handle a situation where you dis...\n","Name: Question, Length: 653, dtype: object"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["ground_truth1=ground_truth_questions.loc[ground_truth_questions['Job Position']==job_position[1],'Question']\n","ground_truth1"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T06:25:30.203212Z","iopub.status.busy":"2024-04-10T06:25:30.202455Z","iopub.status.idle":"2024-04-10T06:28:07.865552Z","shell.execute_reply":"2024-04-10T06:28:07.864310Z","shell.execute_reply.started":"2024-04-10T06:25:30.203178Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","1. Can you tell me about your experience as a nursing assistant?\n","2. What motivated you to become a nursing assistant?\n","3. How do you handle difficult situations with patients or colleagues?\n","4. Can you give an example of a time when you went above and beyond to provide quality care for a patient?\n","5. How do you stay up-to-date with the latest medical advancements and practices?\n","6. How do you prioritize tasks and manage your time effectively as a\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","1. Can you tell me about a time when you had to handle a difficult patient?\n","2. How did you manage the situation?\n","3. What steps did you take to ensure that patient care remained the top priority?\n","4. Did you seek guidance from supervisors or colleagues?\n","5. How did you address the patient's concerns?\n","6. How did you handle any emotional escalation during the situation?\n","7. Can you give an example of a time when you had to handle\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Interviewer: Can you tell me about a time when you had to deal with a difficult patient?\n","Candidate: Yes, I had a patient who was very rude and disrespectful towards me and the other staff members. I tried to remain calm and professional, but it was challenging.\n","Interviewer: How did you handle the situation?\n","Candidate: I tried to understand the patient's perspective and address their concerns. I also communicated clearly and respectfully with them, and tried to find a solution\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","1. Can you tell me about a time when you had to do something difficult?\n","2. How did you handle that situation?\n","3. Can you give an example of a time when you had to mediate a conflict within your team?\n","4. How did you facilitate open communication during the conflict?\n","5. How did you mediate discussions to find a collaborative solution?\n","6. How can this experience help you as a Certified nursing assistant?\n","7. How do you prioritize\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","1. Can you discuss the challenges of working with big data and techniques to handle scalability, performance, and resource constraints?\n","2. How do you ensure data quality and consistency when working with big data?\n","3. What are some common data integration challenges when working with big data?\n","4. Can you explain the concept of data partitioning and how it helps with big data processing?\n","5. How do you handle data privacy and security concerns when working with big data?\n","6.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ground_truth_response</th>\n","      <th>original_model_response</th>\n","      <th>peft_model_response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>790             why did you decide on a career...</td>\n","      <td>\\n1. Why did you choose to become a nursing as...</td>\n","      <td>\\n1. Can you tell me about your experience as ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>790             why did you decide on a career...</td>\n","      <td>\\n1. Can you tell me about a time when you had...</td>\n","      <td>\\n1. Can you tell me about a time when you had...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>832      why would you be the best candidate t...</td>\n","      <td>\\nInterviewer: Can you tell me about a time wh...</td>\n","      <td>\\nInterviewer: Can you tell me about a time wh...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>790             why did you decide on a career...</td>\n","      <td>\\n1. Can you tell me about a time when you had...</td>\n","      <td>\\n1. Can you tell me about a time when you had...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2127    What programming languages are you pro...</td>\n","      <td>\\n1. Can you discuss the challenges of working...</td>\n","      <td>\\n1. Can you discuss the challenges of working...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               ground_truth_response  \\\n","0  790             why did you decide on a career...   \n","1  790             why did you decide on a career...   \n","2  832      why would you be the best candidate t...   \n","3  790             why did you decide on a career...   \n","4  2127    What programming languages are you pro...   \n","\n","                             original_model_response  \\\n","0  \\n1. Why did you choose to become a nursing as...   \n","1  \\n1. Can you tell me about a time when you had...   \n","2  \\nInterviewer: Can you tell me about a time wh...   \n","3  \\n1. Can you tell me about a time when you had...   \n","4  \\n1. Can you discuss the challenges of working...   \n","\n","                                 peft_model_response  \n","0  \\n1. Can you tell me about your experience as ...  \n","1  \\n1. Can you tell me about a time when you had...  \n","2  \\nInterviewer: Can you tell me about a time wh...  \n","3  \\n1. Can you tell me about a time when you had...  \n","4  \\n1. Can you discuss the challenges of working...  "]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","original_model_response = []\n","ground_truth_response = []\n","peft_model_response = []\n","\n","\n","\n","for idx, question in enumerate(questions):\n","    #human_baseline_text_output = human_baseline_summaries[idx]\n","    ground_truth=ground_truth_questions.loc[ground_truth_questions['Job Position']==job_position[idx],'Question']\n","    prompt = f\"Instruct: You are an excellent interviewer for the given job role {job_position[idx]} and your task is to ask relevant questions to the candidate like {questions[idx]} but not exactly same.You have to also rate the answer of the candidate like {answer[idx]} like {answer_quality[idx]} and give a summary \\nOutput:\\n\"\n","    \n","    original_model_res = gen(original_model,prompt,500,)\n","    original_model_text_output = original_model_res[0].split('Output:')[1]\n","    \n","    peft_model_res = gen(ft_model,prompt,100,)\n","    peft_model_output = peft_model_res[0].split('Output:')[1]\n","    print(peft_model_output)\n","    peft_model_text_output, success, result = peft_model_output.partition('#End')\n","    \n","\n","    original_model_response.append(original_model_text_output)\n","    peft_model_response.append(peft_model_text_output)\n","    ground_truth_response.append(ground_truth)\n","\n","zipped_response = list(zip(ground_truth_response,original_model_response, peft_model_response))\n"," \n","df = pd.DataFrame(zipped_response, columns = ['ground_truth_response','original_model_response', 'peft_model_response'])\n","df"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T06:05:00.877767Z","iopub.status.busy":"2024-04-10T06:05:00.876658Z","iopub.status.idle":"2024-04-10T06:05:00.886228Z","shell.execute_reply":"2024-04-10T06:05:00.885121Z","shell.execute_reply.started":"2024-04-10T06:05:00.877729Z"},"trusted":true},"outputs":[{"data":{"text/plain":["original_model_response    \\n1. Can you tell me about your experience as ...\n","peft_model_response        \\n1. Why did you choose to become a nursing as...\n","Name: 0, dtype: object"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["df.iloc[0]"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T05:55:16.184069Z","iopub.status.busy":"2024-04-10T05:55:16.183596Z","iopub.status.idle":"2024-04-10T05:55:33.653356Z","shell.execute_reply":"2024-04-10T05:55:33.651925Z","shell.execute_reply.started":"2024-04-10T05:55:16.183990Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c333764a4ef5ebbe3b66bf2ca8f34a7728f382a47e946ef764606600afcd1f90\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install rouge_score"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T06:31:16.104855Z","iopub.status.busy":"2024-04-10T06:31:16.104392Z","iopub.status.idle":"2024-04-10T06:32:06.228976Z","shell.execute_reply":"2024-04-10T06:32:06.227708Z","shell.execute_reply.started":"2024-04-10T06:31:16.104825Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34538224fa38458abcb43831a869d4c3","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ORIGINAL MODEL:\n","{'rouge1': 0.20385520948369024, 'rouge2': 0.12328301886792455, 'rougeL': 0.171788411818597, 'rougeLsum': 0.1744837053002191}\n","PEFT MODEL:\n","{'rouge1': 0.38164692030318637, 'rouge2': 0.2662982871791761, 'rougeL': 0.3400653430626621, 'rougeLsum': 0.34043424152252316}\n"]}],"source":["import evaluate\n","\n","rouge = evaluate.load('rouge')\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_response,\n","    references=ground_truth_response[0:len(original_model_response)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=peft_model_response,\n","    references=ground_truth_response[0:len(peft_model_response)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('PEFT MODEL:')\n","print(peft_model_results)"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T06:32:32.600939Z","iopub.status.busy":"2024-04-10T06:32:32.600123Z","iopub.status.idle":"2024-04-10T06:32:32.608057Z","shell.execute_reply":"2024-04-10T06:32:32.606843Z","shell.execute_reply.started":"2024-04-10T06:32:32.600900Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n","rouge1: 17.78%\n","rouge2: 14.30%\n","rougeL: 16.83%\n","rougeLsum: 16.60%\n"]}],"source":["print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n","\n","improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n","for key, value in zip(peft_model_results.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4763101,"sourceId":8072103,"sourceType":"datasetVersion"},{"datasetId":4768452,"sourceId":8079440,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":47.590297,"end_time":"2024-01-20T10:21:41.280535","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-20T10:20:53.690238","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}
