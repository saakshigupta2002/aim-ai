{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01d14c25100541499c24f960a14ffe75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0324fd8db01a4b9ea8911a885dc5fa6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0821a73281a044488d342ce77811aa8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1967b434eb7e4b7384742c133be88a4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19ae477467914cfd84e9910f570c5918":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c05b2dd78b945bfb79fbf1d589e5a30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deec6ac613f94bcd8613543fd70538b9","IPY_MODEL_d901403b2d0c4493b025ff493189535f","IPY_MODEL_8ba953bb631b4dc7bb6c04ebc0322c7f"],"layout":"IPY_MODEL_6a834fd89b524916ad7835acaffbc15a"}},"230e0af0dd44484da484acdb827bafda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25f70d4326f24de6a5be79833e8dae6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"271968f8d1ab424bb22e019b3918f572":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2edc39ab7cac4384a1c4d9343bba711c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b20fbd056bf4a0f9e461f6560c86ba9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42e429997b7748e083fc11ffa6ceeb06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c05a1251826b4426aef3beb231f6bc20","IPY_MODEL_495bf6bc28eb4557a261fa45241531e8","IPY_MODEL_9ab123fe278c4458a36af26e3b4d4c12"],"layout":"IPY_MODEL_2edc39ab7cac4384a1c4d9343bba711c"}},"495bf6bc28eb4557a261fa45241531e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_748bbaba62c0488cb0d4b769b363e4ff","max":990345061,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55144e5c2b9f45e7a0c8e52ede5ae2e1","value":990345061}},"4f6846868bfc4e009d4182d2b0cbff9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f7f5f316583498ba0182adbf2eb3ecf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"532c03e00ff947f88006d24dd61834fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f82cd7f7b8f54ecf8e7c10261426005e","IPY_MODEL_b33377bd5b1e4863b3cb524d3569393f","IPY_MODEL_f17e2fefb943492faf0bb8ab29f28c29"],"layout":"IPY_MODEL_813995ad728944b6b11391d68665c2d9"}},"55144e5c2b9f45e7a0c8e52ede5ae2e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"557c4125b05d4a2bb5aba9c3a35cdc76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_01d14c25100541499c24f960a14ffe75","style":"IPY_MODEL_8b5c4f27115d422caf9488504dccf6ff","value":true}},"60bbd41a9e594913be07404d15d14e7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ba59663d924c3a82ac769a3539d063":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a834fd89b524916ad7835acaffbc15a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d02d8d162b542c9b0ffc79cf2280f52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"748bbaba62c0488cb0d4b769b363e4ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7558a44baf9a41bab5fbf1a455128b96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c5dee7e66f74798b82c6f4dbdef1de7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d7c62ffc8734a3fb5b571cd24b47855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_d3bb89745ef04aca8c8d439549e5e609","style":"IPY_MODEL_ec13ab385c52454facea3a3b373d588c","tooltip":""}},"813995ad728944b6b11391d68665c2d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8900e91e67e546769eccfbc380b4dc5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89eed8f1703442058ed94454b349ef7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_b84ea5c4fbd240d299450d3bace12e69","IPY_MODEL_c526b0d3a74d4e7db512925a35f10c8e","IPY_MODEL_557c4125b05d4a2bb5aba9c3a35cdc76","IPY_MODEL_7d7c62ffc8734a3fb5b571cd24b47855","IPY_MODEL_c4a8e0e397d245ec8c33699bbc0fc134"],"layout":"IPY_MODEL_4f7f5f316583498ba0182adbf2eb3ecf"}},"8b5c4f27115d422caf9488504dccf6ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ba953bb631b4dc7bb6c04ebc0322c7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_271968f8d1ab424bb22e019b3918f572","placeholder":"​","style":"IPY_MODEL_7558a44baf9a41bab5fbf1a455128b96","value":" 147/147 [00:00&lt;00:00, 3.79kB/s]"}},"9ab123fe278c4458a36af26e3b4d4c12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d02d8d162b542c9b0ffc79cf2280f52","placeholder":"​","style":"IPY_MODEL_bb2bc7d5a2b6462893179e8fd3e1e1df","value":" 990M/990M [00:09&lt;00:00, 112MB/s]"}},"9de81bae9c9c45f5bf05a971c9b95019":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7b079c5f8fa495ca6f1aea0cf26e401":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae1812b154064d768ae0f1cb88c38ac5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b33377bd5b1e4863b3cb524d3569393f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25f70d4326f24de6a5be79833e8dae6b","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8900e91e67e546769eccfbc380b4dc5e","value":200}},"b84ea5c4fbd240d299450d3bace12e69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de2c446320864857b7f0f5b73c681552","placeholder":"​","style":"IPY_MODEL_230e0af0dd44484da484acdb827bafda","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"bb2bc7d5a2b6462893179e8fd3e1e1df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bde9c0b1b7d6473282763191f281b86d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c05a1251826b4426aef3beb231f6bc20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7b079c5f8fa495ca6f1aea0cf26e401","placeholder":"​","style":"IPY_MODEL_0324fd8db01a4b9ea8911a885dc5fa6b","value":"model.safetensors: 100%"}},"c4a8e0e397d245ec8c33699bbc0fc134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ae477467914cfd84e9910f570c5918","placeholder":"​","style":"IPY_MODEL_4f6846868bfc4e009d4182d2b0cbff9f","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"c526b0d3a74d4e7db512925a35f10c8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_9de81bae9c9c45f5bf05a971c9b95019","placeholder":"​","style":"IPY_MODEL_67ba59663d924c3a82ac769a3539d063","value":""}},"d3bb89745ef04aca8c8d439549e5e609":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d901403b2d0c4493b025ff493189535f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0821a73281a044488d342ce77811aa8a","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae1812b154064d768ae0f1cb88c38ac5","value":147}},"de236f6e8c5644589a193429d061f226":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de2c446320864857b7f0f5b73c681552":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deec6ac613f94bcd8613543fd70538b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c5dee7e66f74798b82c6f4dbdef1de7","placeholder":"​","style":"IPY_MODEL_1967b434eb7e4b7384742c133be88a4a","value":"generation_config.json: 100%"}},"ec13ab385c52454facea3a3b373d588c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"f17e2fefb943492faf0bb8ab29f28c29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60bbd41a9e594913be07404d15d14e7b","placeholder":"​","style":"IPY_MODEL_bde9c0b1b7d6473282763191f281b86d","value":" 200/200 [00:00&lt;00:00, 660.64 examples/s]"}},"f82cd7f7b8f54ecf8e7c10261426005e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de236f6e8c5644589a193429d061f226","placeholder":"​","style":"IPY_MODEL_3b20fbd056bf4a0f9e461f6560c86ba9","value":"Map: 100%"}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8072103,"sourceType":"datasetVersion","datasetId":4763101}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning ","metadata":{"id":"_6IgAkg96Wea"}},{"cell_type":"code","source":"# import locale\n# locale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"id":"0sp0x3V-JV_0","execution":{"iopub.status.busy":"2024-04-09T10:14:50.819188Z","iopub.execute_input":"2024-04-09T10:14:50.819925Z","iopub.status.idle":"2024-04-09T10:14:50.824071Z","shell.execute_reply.started":"2024-04-09T10:14:50.819892Z","shell.execute_reply":"2024-04-09T10:14:50.823089Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U datasets scipy","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"q7EV1ZcP6b3F","outputId":"cf635bb9-da64-4385-b58e-32e6e9f8ddfa","execution":{"iopub.status.busy":"2024-04-11T16:44:38.010609Z","iopub.execute_input":"2024-04-11T16:44:38.011365Z","iopub.status.idle":"2024-04-11T16:47:00.824679Z","shell.execute_reply.started":"2024-04-11T16:44:38.011322Z","shell.execute_reply":"2024-04-11T16:47:00.823708Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n# 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\nnotebook_login()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332,"referenced_widgets":["89eed8f1703442058ed94454b349ef7e","b84ea5c4fbd240d299450d3bace12e69","c526b0d3a74d4e7db512925a35f10c8e","557c4125b05d4a2bb5aba9c3a35cdc76","7d7c62ffc8734a3fb5b571cd24b47855","c4a8e0e397d245ec8c33699bbc0fc134","4f7f5f316583498ba0182adbf2eb3ecf","de2c446320864857b7f0f5b73c681552","230e0af0dd44484da484acdb827bafda","9de81bae9c9c45f5bf05a971c9b95019","67ba59663d924c3a82ac769a3539d063","01d14c25100541499c24f960a14ffe75","8b5c4f27115d422caf9488504dccf6ff","d3bb89745ef04aca8c8d439549e5e609","ec13ab385c52454facea3a3b373d588c","19ae477467914cfd84e9910f570c5918","4f6846868bfc4e009d4182d2b0cbff9f"]},"id":"tny1d6XW_YRW","outputId":"132f313d-822a-4b6f-c873-11f3f4ccfedf","execution":{"iopub.status.busy":"2024-04-11T16:47:12.730808Z","iopub.execute_input":"2024-04-11T16:47:12.731753Z","iopub.status.idle":"2024-04-11T16:47:12.758641Z","shell.execute_reply.started":"2024-04-11T16:47:12.731722Z","shell.execute_reply":"2024-04-11T16:47:12.757787Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8fa27ff43d4d4f98083e72c24ddede"}},"metadata":{}}]},{"cell_type":"code","source":"from accelerate import FullyShardedDataParallelPlugin, Accelerator\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n\nfsdp_plugin = FullyShardedDataParallelPlugin(\n    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n)\n\naccelerator = Accelerator(fsdp_plugin=fsdp_plugin)","metadata":{"id":"ISR2nGve6nG9","execution":{"iopub.status.busy":"2024-04-11T16:47:05.603365Z","iopub.execute_input":"2024-04-11T16:47:05.603741Z","iopub.status.idle":"2024-04-11T16:47:11.641132Z","shell.execute_reply.started":"2024-04-11T16:47:05.603710Z","shell.execute_reply":"2024-04-11T16:47:11.640330Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# # If the dataset is gated/private, make sure you have run huggingface-cli login\n\n# dataset = load_dataset(\"akshatshaw/interview\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sw7BIlGS-bVK","outputId":"5c569044-4083-421d-814c-4a5f0ed729fc","execution":{"iopub.status.busy":"2024-04-10T18:57:01.689482Z","iopub.execute_input":"2024-04-10T18:57:01.690584Z","iopub.status.idle":"2024-04-10T18:57:01.699038Z","shell.execute_reply.started":"2024-04-10T18:57:01.690535Z","shell.execute_reply":"2024-04-10T18:57:01.697817Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,AutoModelForSeq2SeqLM\n\nbase_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":99,"referenced_widgets":["42e429997b7748e083fc11ffa6ceeb06","c05a1251826b4426aef3beb231f6bc20","495bf6bc28eb4557a261fa45241531e8","9ab123fe278c4458a36af26e3b4d4c12","2edc39ab7cac4384a1c4d9343bba711c","a7b079c5f8fa495ca6f1aea0cf26e401","0324fd8db01a4b9ea8911a885dc5fa6b","748bbaba62c0488cb0d4b769b363e4ff","55144e5c2b9f45e7a0c8e52ede5ae2e1","6d02d8d162b542c9b0ffc79cf2280f52","bb2bc7d5a2b6462893179e8fd3e1e1df","1c05b2dd78b945bfb79fbf1d589e5a30","deec6ac613f94bcd8613543fd70538b9","d901403b2d0c4493b025ff493189535f","8ba953bb631b4dc7bb6c04ebc0322c7f","6a834fd89b524916ad7835acaffbc15a","7c5dee7e66f74798b82c6f4dbdef1de7","1967b434eb7e4b7384742c133be88a4a","0821a73281a044488d342ce77811aa8a","ae1812b154064d768ae0f1cb88c38ac5","271968f8d1ab424bb22e019b3918f572","7558a44baf9a41bab5fbf1a455128b96"]},"id":"JQcYa3tk_DU6","outputId":"22e97179-ae3d-4d34-a6eb-242b58bf29c7","execution":{"iopub.status.busy":"2024-04-10T18:57:01.701117Z","iopub.execute_input":"2024-04-10T18:57:01.701527Z","iopub.status.idle":"2024-04-10T18:59:27.497241Z","shell.execute_reply.started":"2024-04-10T18:57:01.701482Z","shell.execute_reply":"2024-04-10T18:59:27.496384Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42336d92d23f4d8d8ea05310da1a80fe"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f0787100044c85baef8b779d65db1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9890cb676e9e48a783e3c08da9a5e247"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e054d15885844293a91d0b553724fdd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6053e0e7c324b21a22f80776d624add"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d9c1e7ca0544b1ab308a4f935146919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241b7099716d4df2bc6bc6ad5ed4b3fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e00a8e7fe748f4ab7f93e0a9498cb5"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    model_max_length=512,\n    padding_side=\"left\",\n    add_eos_token=True)\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"0xSZxSs6_lSZ","execution":{"iopub.status.busy":"2024-04-11T16:54:49.113392Z","iopub.execute_input":"2024-04-11T16:54:49.114090Z","iopub.status.idle":"2024-04-11T16:54:49.396702Z","shell.execute_reply.started":"2024-04-11T16:54:49.114059Z","shell.execute_reply":"2024-04-11T16:54:49.395650Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def tokenize(prompt):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"4M6a-asI_DSf","execution":{"iopub.status.busy":"2024-04-11T16:54:52.594072Z","iopub.execute_input":"2024-04-11T16:54:52.594474Z","iopub.status.idle":"2024-04-11T16:54:52.600001Z","shell.execute_reply.started":"2024-04-11T16:54:52.594443Z","shell.execute_reply":"2024-04-11T16:54:52.598982Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import DataCollatorForLanguageModeling\n\ndf1 = pd.read_excel('/kaggle/input/dataset-qa/combined_dataset.xlsx')\ndf2=pd.read_excel('/kaggle/input/dataset-qa/Sde_data.xlsx')\ndf3=pd.read_excel('/kaggle/input/dataset-qa/DS_data.xlsx')\ndf4=pd.read_excel('/kaggle/input/dataset-qa/PMConsult_data.xlsx')\n\ndf1=df1.rename(columns={'Position/Role':'Job_Position'})\n\ndf = pd.concat([df1, df2, df3, df4], ignore_index=True)\ndf.drop(columns=['Column1.6','Column1.7'],inplace=True)\n\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\ndf.to_csv('data.csv',index=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nh_wmYfrBnSH","outputId":"ac39bfb9-ceb9-4ec4-f0c4-ff3e82d5d25b","execution":{"iopub.status.busy":"2024-04-10T18:59:28.317106Z","iopub.execute_input":"2024-04-10T18:59:28.317459Z","iopub.status.idle":"2024-04-10T18:59:41.107818Z","shell.execute_reply.started":"2024-04-10T18:59:28.317431Z","shell.execute_reply":"2024-04-10T18:59:41.106662Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-10 18:59:31.983482: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-10 18:59:31.983639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-10 18:59:32.133795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df_qa,test_df_qa=train_test_split(df,random_state=42)\ntrain_df_qa.reset_index(drop = True,inplace = True)\ntest_df_qa.reset_index(drop = True,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.109318Z","iopub.execute_input":"2024-04-10T18:59:41.109771Z","iopub.status.idle":"2024-04-10T18:59:41.132689Z","shell.execute_reply.started":"2024-04-10T18:59:41.109733Z","shell.execute_reply":"2024-04-10T18:59:41.131652Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def custom_data_collator(df):\n    inputs = tokenizer(\n        [f\"\"\"You are now conducting an interview for the {df[\"Job_Position\"]} role.\n        You have asked the candidate the following question:\n          {df['Question']}\n          for which the candidate has give response, you have to now ask him/her a follow up question based on the given response.\n        \"\"\" for example in zip( df)]\n    )\n    \n    labels =tokenizer([f\"Answer: {answer} </s>\" for df['Answer'] in zip(df)])\n    return {\n        'input_ids': inputs['input_ids'],\n        'attention_mask': inputs['attention_mask'],\n        'labels_ids': labels['label_ids']  \n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.134118Z","iopub.execute_input":"2024-04-10T18:59:41.134553Z","iopub.status.idle":"2024-04-10T18:59:41.142737Z","shell.execute_reply.started":"2024-04-10T18:59:41.134512Z","shell.execute_reply":"2024-04-10T18:59:41.141840Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def compute_input_and_labels2(examples):\n    job_positions, interview_phases, questions, answers = zip(*examples)\n    inputs = [f\"\"\"\n  You are now conducting an interview for the {df[\"Job_Position\"]} role.\n  \n  You have asked the candidate the following question:\n  {df['Question']}\n  \n  The candidate has responded as follows:\n  {df['Answer']}\n  \n  Please formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience. \n  Additionally, provide a comprehensive assessment of the accuracy, clarity, comprehensiveness, and relevance of the candidate's response in relation to the desired skills \n  and knowledge for the {df[\"Job Position\"]} role.\n  <eos>\n  \"\"\"\n              \n    for job_pos, interview_phase, question in zip(job_positions, interview_phases, questions)]\n    labels = [f\"answer: {answer} </s>\" for answer in answers]\n    \n    input_ids = tokenizer(inputs, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    label_ids = tokenizer(labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    return {\"input_ids\": input_ids, \"labels\": label_ids}","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.143935Z","iopub.execute_input":"2024-04-10T18:59:41.144286Z","iopub.status.idle":"2024-04-10T18:59:41.154460Z","shell.execute_reply.started":"2024-04-10T18:59:41.144253Z","shell.execute_reply":"2024-04-10T18:59:41.153664Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_df_qa.to_csv(\"train_split.csv\", index = False)\ntest_df_qa.to_csv(\"test_split.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.157479Z","iopub.execute_input":"2024-04-10T18:59:41.157825Z","iopub.status.idle":"2024-04-10T18:59:41.221681Z","shell.execute_reply.started":"2024-04-10T18:59:41.157799Z","shell.execute_reply":"2024-04-10T18:59:41.220973Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def combine(df):\n    prompt= f\"\"\" You are now conducting an interview for the {df['Job_Position']} role.\n    You have asked the candidate the following question:\n  {df['Question']}\\\n    \n    The candidate has responded as follows:\n  {df['Answer']}\\\n  ask him a follow up question.\n    \"\"\"\n    full = tokenize(prompt)\n    \n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.222688Z","iopub.execute_input":"2024-04-10T18:59:41.222964Z","iopub.status.idle":"2024-04-10T18:59:41.227715Z","shell.execute_reply.started":"2024-04-10T18:59:41.222940Z","shell.execute_reply":"2024-04-10T18:59:41.226828Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def generate_and_tokenize_prompt(df):\n    full_prompt = f\"\"\"\n  You are now conducting an interview for the {df['Job Position']}\\ role.\n  \n  You have asked the candidate the following question:\n  {df['Question']}\\\n  \n  The candidate has responded as follows:\n  {df['Answer']}\\\n  Please formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience of the candidate's response in relation to the desired skills and knowledge for the {df[\"Job Position\"]}\\ role.\n  Outuput:\n  \"\"\"\n    df['prompt'] = full_prompt\n    return tokenize(full_prompt)","metadata":{"id":"GeptANLS_DP5","execution":{"iopub.status.busy":"2024-04-11T17:05:28.456778Z","iopub.execute_input":"2024-04-11T17:05:28.457143Z","iopub.status.idle":"2024-04-11T17:05:28.462692Z","shell.execute_reply.started":"2024-04-11T17:05:28.457116Z","shell.execute_reply":"2024-04-11T17:05:28.461745Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"akshatshaw/eddcv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:05:29.382115Z","iopub.execute_input":"2024-04-11T17:05:29.382951Z","iopub.status.idle":"2024-04-11T17:05:30.319395Z","shell.execute_reply.started":"2024-04-11T17:05:29.382919Z","shell.execute_reply":"2024-04-11T17:05:30.318488Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:05:30.321008Z","iopub.execute_input":"2024-04-11T17:05:30.321305Z","iopub.status.idle":"2024-04-11T17:05:30.327290Z","shell.execute_reply.started":"2024-04-11T17:05:30.321280Z","shell.execute_reply":"2024-04-11T17:05:30.326153Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Job_Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality', 'Job Position'],\n        num_rows: 1742\n    })\n    test: Dataset({\n        features: ['Job_Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality', 'Job Position'],\n        num_rows: 581\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_train_dataset = dataset['train'].map(generate_and_tokenize_prompt)\ntokenized_val_dataset = dataset['test'].map(generate_and_tokenize_prompt)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["532c03e00ff947f88006d24dd61834fc","f82cd7f7b8f54ecf8e7c10261426005e","b33377bd5b1e4863b3cb524d3569393f","f17e2fefb943492faf0bb8ab29f28c29","813995ad728944b6b11391d68665c2d9","de236f6e8c5644589a193429d061f226","3b20fbd056bf4a0f9e461f6560c86ba9","25f70d4326f24de6a5be79833e8dae6b","8900e91e67e546769eccfbc380b4dc5e","60bbd41a9e594913be07404d15d14e7b","bde9c0b1b7d6473282763191f281b86d"]},"id":"GKL-jhTX_DLd","outputId":"1466b976-2163-4876-db56-db3e09b32430","execution":{"iopub.status.busy":"2024-04-11T17:05:30.581852Z","iopub.execute_input":"2024-04-11T17:05:30.582749Z","iopub.status.idle":"2024-04-11T17:05:32.675096Z","shell.execute_reply.started":"2024-04-11T17:05:30.582714Z","shell.execute_reply":"2024-04-11T17:05:32.674213Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1742 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3cb49ccdbb443f7a7323dd7e0dd926d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/581 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85735480f19e429e999a009cc503bef9"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textwrap import wrap\ndef print_wrapped_text(text, width=80):\n    wrapped_text = wrap(text, width=width)\n    for line in wrapped_text:\n        print(line)","metadata":{"id":"I60hTJ_VEE1M","execution":{"iopub.status.busy":"2024-04-10T19:02:09.661478Z","iopub.execute_input":"2024-04-10T19:02:09.661975Z","iopub.status.idle":"2024-04-10T19:02:09.667935Z","shell.execute_reply.started":"2024-04-10T19:02:09.661940Z","shell.execute_reply":"2024-04-10T19:02:09.666678Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"id":"uWcHJs5w_DB-","execution":{"iopub.status.busy":"2024-04-10T19:02:10.326684Z","iopub.execute_input":"2024-04-10T19:02:10.327419Z","iopub.status.idle":"2024-04-10T19:02:10.343290Z","shell.execute_reply.started":"2024-04-10T19:02:10.327382Z","shell.execute_reply":"2024-04-10T19:02:10.342388Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * (trainable_params / all_param)}\"\n    )","metadata":{"id":"DOaJXM9vG8Xl","execution":{"iopub.status.busy":"2024-04-10T19:02:11.310154Z","iopub.execute_input":"2024-04-10T19:02:11.310537Z","iopub.status.idle":"2024-04-10T19:02:11.316268Z","shell.execute_reply.started":"2024-04-10T19:02:11.310507Z","shell.execute_reply":"2024-04-10T19:02:11.315279Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)\n\n# Apply the accelerator. You can comment this out to remove the accelerator.\nmodel = accelerator.prepare_model(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDaVafdeG8QK","outputId":"b7db0e5f-649e-42a5-841e-27791c03b400","execution":{"iopub.status.busy":"2024-04-10T19:02:12.330481Z","iopub.execute_input":"2024-04-10T19:02:12.330886Z","iopub.status.idle":"2024-04-10T19:02:12.902562Z","shell.execute_reply.started":"2024-04-10T19:02:12.330855Z","shell.execute_reply":"2024-04-10T19:02:12.901608Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.device_count() > 1: # If more than 1 GPU\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"id":"FRX9OwjOG8Ny","execution":{"iopub.status.busy":"2024-04-10T19:02:13.864277Z","iopub.execute_input":"2024-04-10T19:02:13.864687Z","iopub.status.idle":"2024-04-10T19:02:13.869527Z","shell.execute_reply.started":"2024-04-10T19:02:13.864638Z","shell.execute_reply":"2024-04-10T19:02:13.868468Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom datetime import datetime\n\nproject = \"interview-finetune\"\nbase_model_name = \"mistral\"\nrun_name = base_model_name + \"-\" + project\noutput_dir = \"./\" + run_name\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args=transformers.TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=2,\n        per_device_train_batch_size=2,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=4,\n        max_steps=50,\n        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n        logging_steps=5,\n        bf16=False,\n        optim=\"paged_adamw_8bit\",\n        tf32=False,\n        logging_dir=\"./logs\",        # Directory for storing logs\n        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n        save_steps=5,                # Save checkpoints every 50 steps\n        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n        eval_steps=5,                # Evaluate and save checkpoints every 50 steps\n        do_eval=True,                # Perform evaluation at the end of training\n        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n# 962f65e904cc5c119e8c9c2b7749e37073e22bce api key","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WhqD_a3GG8LR","outputId":"cd3346eb-2ae1-42d4-98e3-8ad91f6acc26","execution":{"iopub.status.busy":"2024-04-10T19:09:29.043544Z","iopub.execute_input":"2024-04-10T19:09:29.044464Z","iopub.status.idle":"2024-04-10T22:40:55.864939Z","shell.execute_reply.started":"2024-04-10T19:09:29.044429Z","shell.execute_reply":"2024-04-10T22:40:55.863283Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/50 3:28:37 < 1:09:32, 0.00 it/s, Epoch 0.34/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>2.593300</td>\n      <td>2.224325</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.024700</td>\n      <td>1.762077</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.618400</td>\n      <td>1.375493</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.192000</td>\n      <td>1.148024</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.116400</td>\n      <td>1.056012</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.008400</td>\n      <td>1.026231</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.021400</td>\n      <td>1.012818</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 40\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 962f65e904cc5c119e8c9c2b7749e37073e22bce api key\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1849\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2193\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2193\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2196\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2197\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2199\u001b[0m ):\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3137\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3135\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n# 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\n# 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 write access\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:42:03.456574Z","iopub.execute_input":"2024-04-10T22:42:03.456971Z","iopub.status.idle":"2024-04-10T22:42:03.485990Z","shell.execute_reply.started":"2024-04-10T22:42:03.456943Z","shell.execute_reply":"2024-04-10T22:42:03.485010Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"363333db31af420c9f40261b713c19c7"}},"metadata":{}}]},{"cell_type":"code","source":"# from huggingface_hub import create_repo\n# create_repo(\"mistral-interview-finetune-v_1.0\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:45:28.252147Z","iopub.execute_input":"2024-04-10T22:45:28.252521Z","iopub.status.idle":"2024-04-10T22:45:28.258864Z","shell.execute_reply.started":"2024-04-10T22:45:28.252494Z","shell.execute_reply":"2024-04-10T22:45:28.257539Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:54:39.530588Z","iopub.execute_input":"2024-04-10T22:54:39.531415Z","iopub.status.idle":"2024-04-10T22:55:14.182303Z","shell.execute_reply.started":"2024-04-10T22:54:39.531378Z","shell.execute_reply":"2024-04-10T22:55:14.180982Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/609M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1747cfd0b824bed9aa1ea3ffa93d6da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55350cc5835c487c8ba81e4feb23c4f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fb4807e8ad4dc28ddbd4979431bb1d"}},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/akshatshaw/mistral-interview-finetune/commit/29d4dfaa2f08a53dce06301d354dca2a9a289112', commit_message='End of training', commit_description='', oid='29d4dfaa2f08a53dce06301d354dca2a9a289112', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# from huggingface_hub import HfApi\n\n# api=HfApi()\n\n# api.upload_folder(\n#     folder_path=\"/kaggle/working/mistral-interview-finetune/checkpoint-35\",\n#     repo_id=\"mistral-interview-finetune-v_1.0\",\n#     repo_type=\"model\",\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U datasets scipy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbase_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n# AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,  # Mistral, same as before\n    quantization_config=bnb_config,  # Same quantization config as before\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\neval_tokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    add_bos_token=True,\n    trust_remote_code=True,\n)","metadata":{"id":"8iIZqxoFG8G6","execution":{"iopub.status.busy":"2024-04-11T16:47:44.789247Z","iopub.execute_input":"2024-04-11T16:47:44.789584Z","iopub.status.idle":"2024-04-11T16:49:59.942909Z","shell.execute_reply.started":"2024-04-11T16:47:44.789560Z","shell.execute_reply":"2024-04-11T16:49:59.941910Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3de4e950c8314cc1981d02d8406667b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af38a3a47a3b4fab8cca51a12ab3345a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c2864fafd84c9581cf5fa18376065b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e2aede477654f4796c82d06847459dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aea4b2a75e84c2c8277cb0e52e2e17c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea1331f7bcb744ebb5d3fe8c173f4671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4866b65f89429188f8feff655a4d10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64757427d2f946c1aa2c369eab650eb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb48d74c6ab44569b40f04134cda464b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1291517325477a9a7c929ad0750602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07c02e71b5b474f8ae9494af652fc85"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM\n\nconfig = PeftConfig.from_pretrained(\"akshatshaw/mistral-interview-finetune\")\nmodel = PeftModel.from_pretrained(base_model, \"akshatshaw/mistral-interview-finetune\")","metadata":{"id":"3CvTroLYG8E0","execution":{"iopub.status.busy":"2024-04-11T16:52:21.856024Z","iopub.execute_input":"2024-04-11T16:52:21.856703Z","iopub.status.idle":"2024-04-11T16:52:26.401878Z","shell.execute_reply.started":"2024-04-11T16:52:21.856672Z","shell.execute_reply":"2024-04-11T16:52:26.401096Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/751 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c3a3fc73439496cbe6b38ea9b0c5861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/609M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da4031a09c6c46b6942a6454202a9568"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"akshatshaw/eddcv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:53:08.364983Z","iopub.execute_input":"2024-04-11T16:53:08.365720Z","iopub.status.idle":"2024-04-11T16:53:12.780594Z","shell.execute_reply.started":"2024-04-11T16:53:08.365689Z","shell.execute_reply":"2024-04-11T16:53:12.779596Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 821k/821k [00:00<00:00, 3.84MB/s]\nDownloading data: 100%|██████████| 268k/268k [00:00<00:00, 3.04MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8e166a1c5e4d0686018a9437a59106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"062fd14935664a1cac3ce4a782967309"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:53:16.958316Z","iopub.execute_input":"2024-04-11T16:53:16.959166Z","iopub.status.idle":"2024-04-11T16:53:16.965757Z","shell.execute_reply.started":"2024-04-11T16:53:16.959122Z","shell.execute_reply":"2024-04-11T16:53:16.964741Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Job_Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality', 'Job Position'],\n        num_rows: 1742\n    })\n    test: Dataset({\n        features: ['Job_Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality', 'Job Position'],\n        num_rows: 581\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"eval_prompt = \"\"\"\nYou are now conducting an interview for the Customer Service Representative role.\nYou have asked the candidate the following question: Name any Two Improvements You Made in the Previous Company?\nThe candidate has responded as follows: As a few of my team members were late to work, which impacted the work, I changed the login conditions by introducing timely login incentives. Hence, it improved the login time.‚Äù There were a few people who used to miss the calls and sometimes used break aux and stayed on it for longer. Announcing a reward for the highest number of calls attended improved the work balance effectively\nPlease formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience of the candidate's response in relation to the desired skills and knowledge for the Customer Service Representative role.\noutput:\n\"\"\"\n\nmodel_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    output = (tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\nout = output.split(\":\")[-1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JojDNKk8HjoR","outputId":"6ebd5ee7-2af7-4188-91a9-e5a3954337b1","execution":{"iopub.status.busy":"2024-04-10T23:24:28.318139Z","iopub.execute_input":"2024-04-10T23:24:28.318510Z","iopub.status.idle":"2024-04-10T23:24:33.752955Z","shell.execute_reply.started":"2024-04-10T23:24:28.318482Z","shell.execute_reply":"2024-04-10T23:24:33.751992Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nYou are now conducting an interview for the Customer Service Representative role.\nYou have asked the candidate the following question: Name any Two Improvements You Made in the Previous Company?\nThe candidate has responded as follows: As a few of my team members were late to work, which impacted the work, I changed the login conditions by introducing timely login incentives. Hence, it improved the login time.‚Äù There were a few people who used to miss the calls and sometimes used break aux and stayed on it for longer. Announcing a reward for the highest number of calls attended improved the work balance effectively\nPlease formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience of the candidate's response in relation to the desired skills and knowledge for the Customer Service Representative role.\nWhat specific steps did you take to implement these improvements and how did you measure their impact on the team's performance and customer satisfaction?\n","output_type":"stream"}]},{"cell_type":"code","source":"pred = []\nfor items in tokenized_val_dataset['prompt']:\n    model_input = eval_tokenizer(items , return_tensors=\"pt\").to(\"cuda\")\n    model.eval()\n    with torch.no_grad():\n        output = (tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n        out = output.split(\":\")[-1]\n        pred.append(out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:20:15.520252Z","iopub.execute_input":"2024-04-11T18:20:15.521125Z","iopub.status.idle":"2024-04-11T18:20:15.526855Z","shell.execute_reply.started":"2024-04-11T18:20:15.521091Z","shell.execute_reply":"2024-04-11T18:20:15.525895Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"430"},"metadata":{}}]},{"cell_type":"code","source":"len(tokenized_val_dataset['Question'][:430])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:22:59.100089Z","iopub.execute_input":"2024-04-11T18:22:59.101110Z","iopub.status.idle":"2024-04-11T18:22:59.109877Z","shell.execute_reply.started":"2024-04-11T18:22:59.101073Z","shell.execute_reply":"2024-04-11T18:22:59.108926Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"430"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred[1]","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:26:01.944104Z","iopub.execute_input":"2024-04-11T18:26:01.944582Z","iopub.status.idle":"2024-04-11T18:26:01.951747Z","shell.execute_reply.started":"2024-04-11T18:26:01.944549Z","shell.execute_reply":"2024-04-11T18:26:01.950728Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'  Based on your understanding of the position and the skills required, what makes you uniquely qualified to excel in this role? Please provide specific examples from your experience to support your response.'"},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\n\n# Define the candidate predictions and reference sentences\npredictions = pred\nreferences = tokenized_val_dataset['Question'][:430]\n\n# Load the BLEU evaluation metric\nbleu = evaluate.load(\"bleu\")\n\n# Compute the BLEU score\nresults = bleu.compute(predictions=predictions, references=references)\n\n# Print the results\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:29:30.631135Z","iopub.execute_input":"2024-04-11T18:29:30.632095Z","iopub.status.idle":"2024-04-11T18:29:31.398188Z","shell.execute_reply.started":"2024-04-11T18:29:30.632052Z","shell.execute_reply":"2024-04-11T18:29:31.397292Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"{'bleu': 0.023421683258425006, 'precisions': [0.10455943307928868, 0.030399418366883264, 0.013347545998053482, 0.007093204709887928], 'brevity_penalty': 1.0, 'length_ratio': 4.421083743842365, 'translation_length': 22437, 'reference_length': 5075}\n","output_type":"stream"}]},{"cell_type":"code","source":"bleu = evaluate.load(\"bleu\")\n\n# Compute the BLEU score\nresults = bleu.compute(predictions=predictions, references=references)\n\n# Print the results\nprint(results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:24:48.279996Z","iopub.execute_input":"2024-04-11T18:24:48.280697Z","iopub.status.idle":"2024-04-11T18:25:03.482665Z","shell.execute_reply.started":"2024-04-11T18:24:48.280663Z","shell.execute_reply":"2024-04-11T18:25:03.481625Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c742103bcca630645bb805c6e92778019b8f6aaabdbc3c0d9287a0ad3616f0e9\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\n# Load the ROUGE evaluation metric\nrouge = evaluate.load('rouge')\n\n# Define the candidate predictions and reference sentences\npredictions = pred\nreferences = tokenized_val_dataset['Question'][:430]\n\n# Compute the ROUGE score\nresults = rouge.compute(predictions=predictions, references=references)\n\n# Print the results\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:29:40.437658Z","iopub.execute_input":"2024-04-11T18:29:40.438031Z","iopub.status.idle":"2024-04-11T18:29:41.510785Z","shell.execute_reply.started":"2024-04-11T18:29:40.438001Z","shell.execute_reply":"2024-04-11T18:29:41.509814Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"{'rouge1': 0.1828892019214882, 'rouge2': 0.05595716756740922, 'rougeL': 0.14946545570101497, 'rougeLsum': 0.14924759519559638}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}